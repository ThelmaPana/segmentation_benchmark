---
title: "Statistics for segmentation benchmark"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=10, fig.height=6)
library(tidyverse)
library(scales)
```

The purpose of this script is to compare the performances of two segmentation pipelines against a ground truth human segmentation, on 106 large `apeep` images (10240 px * 2048 px).  
The first pipeline (`regular segmentation`) is an adaptive gray level segmentation, down to 50 px.  
The second pipeline (`semantic segmentation`) is a combination of:  

* an adaptive gray level segmentation for large particles (> 300 px)
* a semantic bbox proposal followed by the same gray level segmentation for small particles (50 px - 300 px) 



## Prepare data
### Read data


```{r read_data}
output_dir <- "data/manual/matches_900_04_V2"

man_parts <- read_csv(file.path(output_dir, "man_particles_props.csv"), col_types = cols()) %>% select(-c("object_label", "object_bbox-0", "object_bbox-1", "object_bbox-2", "object_bbox-3"))
reg_parts <- read_csv(file.path(output_dir, "reg_particles_props.csv"), col_types = cols()) %>% select(-c("object_label", "object_bbox-0", "object_bbox-1", "object_bbox-2", "object_bbox-3"))
sem_parts <- read_csv(file.path(output_dir, "sem_particles_props.csv"), col_types = cols()) %>% select(-c("object_label", "object_bbox-0", "object_bbox-1", "object_bbox-2", "object_bbox-3"))

matches_reg <- read_csv(file.path(output_dir, "matches_reg.csv"), col_types = cols())
matches_sem <- read_csv(file.path(output_dir, "matches_sem.csv"), col_types = cols())
```


### Select relevant objects

Make a list of taxa in manually segmented particles.

```{r taxa}
taxa <- man_parts %>% pull(taxon) %>% unique() %>% sort()
taxa
```

The manual segmentation originally generated `r nrow(man_parts)` particles.
We will ignore objects in the `detritus` and `othertocheck` categories as well as objects smaller than 50 px.

```{r filter_objects}
ignored <- man_parts %>% filter(taxon %in% c("detritus", "othertocheck")) %>% pull(object_id)
small <- man_parts %>% filter(area < 50) %>% pull(object_id)
man_parts <- man_parts %>% filter(!(taxon %in% c("detritus", "othertocheck"))) %>% filter(area >= 50)

matches_sem <- matches_sem %>% filter((!(man_ids %in% ignored)) & (!(man_ids %in% small))) 
matches_reg <- matches_reg %>% filter((!(man_ids %in% ignored)) & (!(man_ids %in% small))) 
```

**After removing non living and small particles, `r nrow(man_parts)` manual particles are left.
`r nrow(reg_parts)` particles were generated by regular segmentation and `r nrow(sem_parts)` particles for semantic segmentation on the same 106 images.**

Let’s inspect taxonomic composition of benchmark dataset.

```{r testset_comp}
man_parts %>% 
  count(taxon) %>% 
  arrange(-n) %>% 
  mutate(taxon = factor(taxon, taxon)) %>% 
  ggplot() +
  geom_col(aes(x = taxon, y = n, fill = n > 10)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(trans = "log1p", breaks = c(0, 10, 50, 100, 200, 400, 1000, 2000)) +
  labs(x = "Taxon", y = "Object number", title = "Test set composition") + 
  theme(text = element_text(size = 16)) 
  
```

```{r testset_comp_small}
man_parts %>% 
  filter(object_area < 200) %>% 
  count(taxon) %>% 
  arrange(-n) %>% 
  mutate(taxon = factor(taxon, taxon)) %>% 
  ggplot() +
  geom_col(aes(x = taxon, y = n, fill = n > 10)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(trans = "log1p", breaks = c(0, 10, 50, 100, 200, 400, 1000, 2000)) +
  labs(x = "Taxon", y = "Object number", title = "Test set composition for small objects (< 300 px)") + 
  theme(text = element_text(size = 16)) 
  
```

Have also a look at training set composition.

```{r training_set}
training_set <- read_csv("data/manual/ecotaxa_export_training_set.csv", col_types = cols()) %>% select(object_id, area, taxon)

training_set %>% 
  filter(area < 300) %>% 
  count(taxon) %>% 
  arrange(-n) %>% 
  mutate(taxon = factor(taxon, taxon)) %>% 
  ggplot() +
  geom_col(aes(x = taxon, y = n, fill = n > 10)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(trans = "log1p", breaks = c(0, 10, 50, 100, 200, 400, 1000, 2000)) +
  labs(x = "Taxon", y = "Object number", title = "Traning set composition for small objects (< 300 px)") + 
  theme(text = element_text(size = 16)) 
```




## Compute global statistics
Compute overall precision and recall:  

* precision: among `apeep` particles, how many where matched with manual particles?
* recall: among manual particles, how many where matched with `apeep` particles?

```{r global}
# Precision
# apeep particles matched with manual particles / apeep particles
precision_reg <- length(unique(matches_reg$reg_ids)) / length(reg_parts$object_id)
precision_sem <- length(unique(matches_sem$sem_ids)) / length(sem_parts$object_id)

# Recall
# manual particles matched with apeep particles / manual particles
recall_reg <- length(unique(matches_reg$man_ids)) / length(man_parts$object_id)
recall_sem <- length(unique(matches_sem$man_ids)) / length(man_parts$object_id)

global_stats <- tibble(
  metric = c("precision", "precision", "recall", "recall"),
  segmentation = c("regular", "semantic", "regular", "semantic"), 
  value = c(precision_reg, precision_sem, recall_reg, recall_sem)
)
global_stats
```

Plot it

```{r global_plot, echo=FALSE}
global_stats %>% 
  ggplot() +
  geom_col(aes(x = metric, y = value, fill = segmentation), position = "dodge") +
  theme(text = element_text(size = 16)) 
```

**Regular segmentation has a very good recall (`r percent(recall_reg, accuracy = 0.1)`) but a very poor precision (`r percent(precision_reg, accuracy = 0.1)`), performing classification on this huge number of particles will be difficult. Semantic segmentation extracts `r percent(recall_sem, accuracy = 0.1)` of relevant particles and only `r percent(1 - precision_sem, accuracy = 0.1)` of extracted particles are not relevant.**


## Compute statistics per taxon
We want to compute the recall of organism per taxonomic group, but we have to deal with multiple matches.

Case 1: one `apeep` particle matched with multiple manual particles, likely with two different taxo.
Two solutions:

- take the rarest taxo
- ignore particle as the CNN won’t be able to predict it (selected solution)

Case 2: one manual particle matched with multiple `apeep` particles, only one taxo but `apeep` segmentation overestimates the number of organisms in this taxo. 

Solution: keep only one match.

```{r taxo_stats}
## Regular particles
# Count matches 
counts_reg_taxo <- matches_reg %>% 
  # drop all cases of duplicated matches of apeep particles in match table (solve case 1 of multiple matches)
  add_count(reg_ids) %>% filter(n==1) %>% select(-n) %>% 
  # drop duplicates of matched manual particles and keep only one (solve case 2 of multiple matches)
  distinct(man_ids, .keep_all = TRUE) %>% 
  # join matched regular particles with manual particles taxo
  left_join(man_parts, by = c("man_ids" = "object_id")) %>% 
  # count matches per taxon
  count(taxon, name = "n_reg") 

# Compute recall
recall_reg_taxo <- man_parts %>% 
  # count true particles per taxon
  count(taxon, name = "n_truth") %>% 
  # join with semantic matched particles
  left_join(counts_reg_taxo, by = "taxon") %>% 
  # compute ratio of regular matched particles over true particles (recall)
  mutate(recall_reg = n_reg / n_truth) 

## Semantic particles
# Count matches 
counts_sem_taxo <- matches_sem %>% 
  # drop all cases of duplicated matches of apeep particles in match table (solve case 1 of multiple matches)
  add_count(sem_ids) %>% filter(n==1) %>% select(-n) %>% 
  # drop duplicates of matched manual particles and keep only one (solve case 2 of multiple matches)
  distinct(man_ids, .keep_all = TRUE) %>% 
  # join matched semantic particles with manual particles taxo
  left_join(man_parts, by = c("man_ids" = "object_id")) %>% 
  # count matches per taxon
  count(taxon, name = "n_sem") 

# Compute recall
recall_sem_taxo <- man_parts %>% 
  # count true particles per taxon
  count(taxon, name = "n_truth") %>% 
   # join with semantic matched particles
  left_join(counts_sem_taxo, by = "taxon") %>%
  # compute ratio of semantic matched particles over true particles (recall)
  mutate(recall_sem = n_sem / n_truth) 


# Join regular and semantic recall data and plot
recall_taxo <- recall_reg_taxo %>% 
  left_join(recall_sem_taxo, by = c("taxon", "n_truth")) %>% 
  select(taxon, n_truth, contains("recall")) %>% 
  arrange(n_truth) %>% 
  #mutate(taxon = factor(taxon, levels = rev(taxon))) %>% 
  mutate(taxon = factor(taxon, taxon)) %>% 
  gather(recall_reg:recall_sem, key = "segmentation", value = "recall") 

# Plot it
recall_taxo %>% 
  ggplot() +
  geom_col(aes(x = taxon, y = n_truth, fill = recall), position = "dodge") +
  scale_fill_viridis_c(direction = -1) + 
  scale_y_continuous(trans = "log1p") +
  coord_flip() +
  facet_wrap(~segmentation,  labeller = as_labeller(c("recall_reg" = "Regular segmentation", "recall_sem" = "Semantic segmentation"))) +
  labs(title = "Recall scores and organisms number per taxa for each segmentation", y = "Number of true objects") +
  theme(text = element_text(size = 16)) 
```

Other plot 

```{r taxo_plot, echo=FALSE}
recall_taxo %>% 
  ggplot() +
  geom_col(aes(x = taxon, y = recall, fill = n_truth)) + 
  scale_fill_viridis_c(trans = "log1p", direction = -1) + 
  coord_flip() +
  facet_wrap(~segmentation, labeller = as_labeller(c("recall_reg" = "Regular segmentation", "recall_sem" = "Semantic segmentation"))) +
  labs(title = "Recall scores and organisms number per taxa for each segmentation", fill = "Number of \ntrue objects") +
  theme(text = element_text(size = 16)) 

```

**Semantic segmentation is less performant on Bacillariophycaea (looks like fibers), like<Copepoda (blurred copepods) and Rhizaria (can be similar to small detritus).**

Let’s focus only on objects smaller than 300 px.

```{r taxo_stats_large}
## Regular particles
# Count matches 
counts_reg_taxo <- matches_reg %>% 
  # drop all cases of duplicated matches of apeep particles in match table (solve case 1 of multiple matches)
  add_count(reg_ids) %>% filter(n==1) %>% select(-n) %>% 
  # drop duplicates of matched manual particles and keep only one (solve case 2 of multiple matches)
  distinct(man_ids, .keep_all = TRUE) %>% 
  # join matched regular particles with manual particles taxo
  left_join(man_parts, by = c("man_ids" = "object_id")) %>% 
  # keep only particles smaller than 300 px
  filter(area < 300) %>% 
  # count matches per taxon
  count(taxon, name = "n_reg") 

# Compute recall
recall_reg_taxo <- man_parts %>% 
  # keep only particles smaller than 300 px
  filter(area < 300) %>% 
  # count true particles per taxon
  count(taxon, name = "n_truth") %>% 
  # join with semantic matched particles
  left_join(counts_reg_taxo, by = "taxon") %>% 
  # compute ratio of regular matched particles over true particles (recall)
  mutate(recall_reg = n_reg / n_truth) 

## Semantic particles
# Count matches 
counts_sem_taxo <- matches_sem %>% 
  # drop all cases of duplicated matches of apeep particles in match table (solve case 1 of multiple matches)
  add_count(sem_ids) %>% filter(n==1) %>% select(-n) %>% 
  # drop duplicates of matched manual particles and keep only one (solve case 2 of multiple matches)
  distinct(man_ids, .keep_all = TRUE) %>% 
  # join matched semantic particles with manual particles taxo
  left_join(man_parts, by = c("man_ids" = "object_id")) %>% 
  # keep only particles smaller than 300 px
  filter(area < 300) %>% 
  # count matches per taxon
  count(taxon, name = "n_sem") 

# Compute recall
recall_sem_taxo <- man_parts %>% 
  # keep only particles smaller than 300 px
  filter(area < 300) %>% 
  # count true particles per taxon
  count(taxon, name = "n_truth") %>% 
   # join with semantic matched particles
  left_join(counts_sem_taxo, by = "taxon") %>%
  # compute ratio of semantic matched particles over true particles (recall)
  mutate(recall_sem = n_sem / n_truth) 


# Join regular and semantic recall data and plot
recall_taxo <- recall_reg_taxo %>% 
  left_join(recall_sem_taxo, by = c("taxon", "n_truth")) %>% 
  select(taxon, n_truth, contains("recall")) %>% 
  arrange(n_truth) %>% 
  mutate(taxon = factor(taxon, taxon)) %>% 
  gather(recall_reg:recall_sem, key = "segmentation", value = "recall") %>% 
  replace_na(list(recall = 0))

# Plot it
recall_taxo %>% 
  ggplot() +
  geom_col(aes(x = taxon, y = n_truth, fill = recall), position = "dodge") +
  scale_fill_viridis_c(direction = -1) + 
  scale_y_continuous(trans = "log1p") +
  coord_flip() +
  facet_wrap(~segmentation,  labeller = as_labeller(c("recall_reg" = "Regular segmentation", "recall_sem" = "Semantic segmentation"))) +
  labs(title = "Recall scores and organisms number per taxa for each segmentation on small objects", y = "Number of true objects") +
  theme(text = element_text(size = 16)) 
```


Other plot 

```{r taxo_plot_large, echo=FALSE}
recall_taxo %>% 
  ggplot() +
  geom_col(aes(x = taxon, y = recall, fill = n_truth)) + 
  scale_fill_viridis_c(trans = "log1p", direction = -1) + 
  coord_flip() +
  facet_wrap(~segmentation, labeller = as_labeller(c("recall_reg" = "Regular segmentation", "recall_sem" = "Semantic segmentation"))) +
  labs(title = "Recall scores and organisms number per taxa for each segmentation for small objects", fill = "Number of \ntrue objects") +
  theme(text = element_text(size = 16)) 

```

## Compute statistics per size class

We will define size classes for particles:

* [50 px,100 px)  
* [100 px, 150 px) 
* [150 px, 200 px) 
* [200 px, 250 px) 
* [250 px, 300 px) 
* [300 px, 350 px) 
* [350 px, 400 px) 
* [400 px, 450 px) 
* [450 px, 500 px) 
* \> 500 px

And compute recall for each size class.


```{r size_stats}
# Define size classes
# - 50 px from 50 px to 500 px
# - larger than 500 px
man_parts <- man_parts %>% 
  mutate(class_size = cut(area, breaks = c(0, seq(from = 50, to = 500, by = 50), 1000000), right = FALSE))

## Regular particles
# Count matches 
counts_reg_size <- matches_reg %>% 
  # drop all cases of duplicated matches of apeep particles in match table (solve case 1 of multiple matches)
  add_count(reg_ids) %>% filter(n==1) %>% select(-n) %>% 
  # drop duplicates of matched manual particles and keep only one (solve case 2 of multiple matches)
  distinct(man_ids, .keep_all = TRUE) %>% 
  # join matched regular particles with manual particles class size
  left_join(man_parts, by = c("man_ids" = "object_id")) %>% 
   # count matches per class size
  count(class_size, name = "n_reg")

# Compute recall
recall_reg_size <- man_parts %>% 
  # count true particles per class size
  count(class_size, name = "n_truth") %>% 
  # join with semantic matched particles
  left_join(counts_reg_size, by = "class_size") %>% 
  # compute ratio of regular matched particles over true particles (recall)
  mutate(recall_reg = n_reg / n_truth) 


## Semantic particles
# Count matches 
counts_sem_size <- matches_sem %>% 
   # drop all cases of duplicated matches of apeep particles in match table (solve case 1 of multiple matches)
  add_count(sem_ids) %>% filter(n==1) %>% select(-n) %>%
  # drop duplicates of matched manual particles and keep only one (solve case 2 of multiple matches)
  distinct(man_ids, .keep_all = TRUE) %>% 
  # join matched semantic particles with manual particles class size
  left_join(man_parts, by = c("man_ids" = "object_id")) %>% 
  # count matches per class size
  count(class_size, name = "n_sem")

# Compute recall
recall_sem_size <- man_parts %>% 
  # count true particles per class size
  count(class_size, name = "n_truth") %>% 
  # join with semantic matched particles
  left_join(counts_sem_size, by = "class_size") %>% 
  # compute ratio of semantic matched particles over true particles (recall)
  mutate(recall_sem = n_sem / n_truth) 


# Join regular and semantic recall data and plot
recall_size <- recall_reg_size %>% 
  left_join(recall_sem_size, by = c("class_size", "n_truth")) %>% 
  select(class_size, n_truth, contains("recall")) %>% 
  gather(recall_reg:recall_sem, key = "segmentation", value = "recall") 

recall_size %>% 
  ggplot() +
  geom_col(aes(x = class_size, y = n_truth, fill = recall), position = "dodge") +
  scale_fill_viridis_c(direction = -1) + 
  scale_y_continuous(trans = "log1p") +
  coord_flip() +
  facet_wrap(~segmentation, labeller = as_labeller(c("recall_reg" = "Regular segmentation", "recall_sem" = "Semantic segmentation"))) +
  labs(title = "Recall scores and organisms number per size class for each segmentation", fill = "Recall", 
       x = "Size class (px)", y = "Number of \ntrue objects") +
  theme(text = element_text(size = 16)) 
```

Other plot.

```{r size_plot, echo=FALSE}
recall_size %>% 
  ggplot() +
  geom_col(aes(x = class_size, y = recall, fill = n_truth)) + 
  scale_fill_viridis_c(trans = "log1p", direction = -1) + 
  coord_flip() +
  facet_wrap(~segmentation, labeller = as_labeller(c("recall_reg" = "Regular segmentation", "recall_sem" = "Semantic segmentation"))) +
  labs(title = "Recall scores and organisms number per size class for each segmentation", fill = "Number of \ntrue objects", x = "Size class (px)") +
  theme(text = element_text(size = 16)) 

```

**Semantic segmentation is less performant on small (< 50 px) classes.**


Now compute precision on each size class.

```{r size_prec}
# Define size classes
# - 50 px from 50 px to 500 px
# - larger than 500 px
reg_parts <- reg_parts %>% mutate(class_size = cut(object_area, breaks = c(0, seq(from = 50, to = 500, by = 50), 1000000), right = FALSE))
sem_parts <- sem_parts %>% mutate(class_size = cut(object_area, breaks = c(0, seq(from = 50, to = 500, by = 50), 1000000), right = FALSE))

## Regular particles
# Count matches 
counts_reg_size <- matches_reg %>% 
  # drop all cases of duplicated matches of apeep particles in match table (solve case 1 of multiple matches)
  add_count(reg_ids) %>% filter(n==1) %>% select(-n) %>% 
  # drop duplicates of matched manual particles and keep only one (solve case 2 of multiple matches)
  distinct(man_ids, .keep_all = TRUE) %>% 
  # join matched regular particles with regular particles class size
  left_join(reg_parts, by = c("reg_ids" = "object_id")) %>% 
   # count matches per class size
  count(class_size, name = "n_match")

# Compute recall
precision_reg_size <- reg_parts %>% 
  # count true particles per class size
  count(class_size, name = "n_reg") %>% 
  # join with semantic matched particles
  left_join(counts_reg_size, by = "class_size") %>% 
  # compute ratio of regular matched particles over true particles (recall)
  mutate(precision_reg = n_match / n_reg) 


## Semantic particles
# Count matches 
counts_sem_size <- matches_sem %>% 
   # drop all cases of duplicated matches of apeep particles in match table (solve case 1 of multiple matches)
  add_count(sem_ids) %>% filter(n==1) %>% select(-n) %>%
  # drop duplicates of matched manual particles and keep only one (solve case 2 of multiple matches)
  distinct(man_ids, .keep_all = TRUE) %>% 
  # join matched semantic particles with semantic particles class size
  left_join(sem_parts, by = c("sem_ids" = "object_id")) %>% 
  # count matches per class size
  count(class_size, name = "n_match")

# Compute recall
precision_sem_size <- sem_parts %>% 
  # count true particles per class size
  count(class_size, name = "n_sem") %>% 
  # join with semantic matched particles
  left_join(counts_sem_size, by = "class_size") %>% 
  # compute ratio of semantic matched particles over true particles (recall)
  mutate(precision_sem = n_match / n_sem) 


# Join regular and semantic recall data and plot
precision_size <- precision_reg_size %>% 
  select(-c(n_reg, n_match)) %>% 
  left_join(precision_sem_size %>% select(-c(n_sem, n_match)), by = "class_size") %>% 
  left_join(man_parts %>% count(class_size, name = "n_truth"),  by = "class_size") %>% 
  select(class_size, n_truth, contains("precision")) %>% 
  gather(precision_reg:precision_sem, key = "segmentation", value = "precision") 

precision_size %>% 
  ggplot() +
  geom_col(aes(x = class_size, y = n_truth, fill = precision), position = "dodge") +
  scale_fill_viridis_c(direction = -1) + 
  scale_y_continuous(trans = "log1p") +
  coord_flip() +
  facet_wrap(~segmentation, labeller = as_labeller(c("precision_reg" = "Regular segmentation", "precision_sem" = "Semantic segmentation"))) +
  labs(title = "Precision scores and organisms number per size class for each segmentation", fill = "Precision", 
       x = "Size class (px)", y = "Number of \ntrue objects") +
  theme(text = element_text(size = 16)) 
```


Other plot.

```{r size_prec_plot, echo=FALSE}
precision_size %>% 
  ggplot() +
  geom_col(aes(x = class_size, y = precision, fill = n_truth)) + 
  scale_fill_viridis_c(trans = "log1p", direction = -1) + 
  coord_flip() +
  facet_wrap(~segmentation, labeller = as_labeller(c("precision_reg" = "Regular segmentation", "precision_sem" = "Semantic segmentation"))) +
  labs(title = "Precision scores and organisms number per size class for each segmentation", fill = "Number of \ntrue objects", x = "Size class (px)") +
  theme(text = element_text(size = 16)) 

```



